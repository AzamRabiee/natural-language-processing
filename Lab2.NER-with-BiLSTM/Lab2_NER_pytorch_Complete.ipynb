{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lab2-NER-pytorch-Complete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.4.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "tUC02h2AZxbf"
      },
      "source": [
        "# Recognize named entities on Twitter with LSTMs\n",
        "\n",
        "In this assignment, you will use a recurrent neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. In this task you will experiment to recognize named entities from Twitter.\n",
        "\n",
        "For example, we want to extract persons' and organizations' names from the text. Than for the input text:\n",
        "\n",
        "    Ian Goodfellow works for Google Brain\n",
        "\n",
        "a NER model needs to provide the following sequence of tags:\n",
        "\n",
        "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
        "\n",
        "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n",
        "\n",
        "A solution of the task will be based on neural networks, particularly, on Bi-Directional Long Short-Term Memory Networks (Bi-LSTMs).\n",
        "\n",
        "### Libraries\n",
        "\n",
        "For this task you will need the following libraries:\n",
        " - [pytorch](https://pytorch.org/) — an open-source software library for Machine Intelligence.\n",
        "    \n",
        " - [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfFm1a75Zxbg"
      },
      "source": [
        "# 1. Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXXgDPMWGcv1"
      },
      "source": [
        "### 1.1. Get Resources\n",
        "The following cell will download all data required for this assignment into the folder `./data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OnOcAweZxbh"
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    ! wget https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py -O setup_google_colab.py\n",
        "    import setup_google_colab\n",
        "    setup_google_colab.setup_week2()\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from common.download_utils import download_week2_resources\n",
        "\n",
        "download_week2_resources()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9rmHTRHZxbk"
      },
      "source": [
        "### 1.2. Load the Twitter Named Entity Recognition corpus\n",
        "\n",
        "We will work with a corpus, which contains tweets with NE tags. Every line of a file contains a pair of a token (word/punctuation symbol) and a tag, separated by a whitespace. Different tweets are separated by an empty line.\n",
        "\n",
        "The function *read_data* reads a corpus from the *file_path* and returns two lists: one with tokens and one with the corresponding tags. You need to complete this function by adding a code, which will replace a user's nickname to `<USR>` token and any URL to `<URL>` token. You could think that a URL and a nickname are just strings which start with *http://* or *https://* in case of URLs and a *@* symbol for nicknames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1auNuazZxbl"
      },
      "source": [
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    tweet_tokens = []\n",
        "    tweet_tags = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if tweet_tokens:\n",
        "                tokens.append(tweet_tokens)\n",
        "                tags.append(tweet_tags)\n",
        "            tweet_tokens = []\n",
        "            tweet_tags = []\n",
        "        else:\n",
        "            token, tag = line.split()\n",
        "            # Replace all urls with <URL> token\n",
        "            # Replace all users with <USR> token\n",
        "\n",
        "            if token.lower().startswith(\"http://\") or token.lower().startswith(\"https://\"):\n",
        "              token = \"<URL>\"\n",
        "            if token.startswith(\"@\"):\n",
        "              token = \"<USR>\"\n",
        "                \n",
        "            tweet_tokens.append(token)\n",
        "            tweet_tags.append(tag)\n",
        "            \n",
        "    return tokens, tags"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k47H-84bZxbn"
      },
      "source": [
        "And now we can load three separate parts of the dataset:\n",
        " - *train* data for training the model;\n",
        " - *validation* data for evaluation and hyperparameters tuning;\n",
        " - *test* data for final evaluation of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuzybiCvZxbo"
      },
      "source": [
        "train_tokens, train_tags = read_data('data/train.txt')\n",
        "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
        "test_tokens, test_tags = read_data('data/test.txt')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEvX7kivZxbq"
      },
      "source": [
        "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4thLr1xGTh0",
        "outputId": "6144703e-b618-4adc-c9be-6b75a9c3e06a"
      },
      "source": [
        "print(\"Number of training samples: \\t\\t\", len(train_tokens))\n",
        "print(\"Number of validation samples: \\t\\t\", len(validation_tokens))\n",
        "print(\"Number of test samples: \\t\\t\", len(test_tokens))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: \t\t 5795\n",
            "Number of validation samples: \t\t 724\n",
            "Number of test samples: \t\t 724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssJtjq-AZxbr"
      },
      "source": [
        "for i in range(2):\n",
        "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
        "        print('%s\\t%s' % (token, tag))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK59OTpwZxbt"
      },
      "source": [
        "### 1.3. Prepare dictionaries\n",
        "\n",
        "To train a neural network, we will use two mappings: \n",
        "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
        "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
        "\n",
        "Now you need to implement the function *build_dict* which will return {token or tag}$\\to${index} and vice versa. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo7tkqv8Zxbt"
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfUx5jYZZxbw"
      },
      "source": [
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "        tokens_or_tags: a list of lists of tokens or tags\n",
        "        special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default value 0\n",
        "    # The lambda function gets called whenever it needs a default value.\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    # Create mappings from tokens (or tags) to indices and vice versa.\n",
        "    # At first, add special tokens (or tags) to the dictionaries.\n",
        "    # The first special token must have index 0.\n",
        "    idx = 0\n",
        "    for token in special_tokens:\n",
        "      tok2idx.update({token: idx})\n",
        "      idx2tok.append(token)\n",
        "      idx += 1\n",
        "    \n",
        "    # Mapping tok2idx should contain each token or tag only once. \n",
        "    # To do so, you should:\n",
        "    # 1. extract unique tokens/tags from the tokens_or_tags variable, which is not\n",
        "    #    occur in special_tokens (because they could have non-empty intersection)\n",
        "    # 2. index them (for example, you can add them into the list idx2tok\n",
        "    # 3. for each token/tag save the index into tok2idx).\n",
        "    \n",
        "    for token_list in tokens_or_tags:\n",
        "      for token in token_list:\n",
        "        if token not in tok2idx:\n",
        "          tok2idx.update({token: idx})\n",
        "          idx2tok.append(token)\n",
        "          idx += 1\n",
        "    return tok2idx, idx2tok"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5iG1f1VZxby"
      },
      "source": [
        "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens in our case will be:\n",
        " - `<UNK>` token for out of vocabulary tokens;\n",
        " - `<PAD>` token for padding sentence to the same length when we create batches of sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEswFgQAZxbz"
      },
      "source": [
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries \n",
        "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oM0RI6-Zxb3"
      },
      "source": [
        "### 1.4. Generate batches\n",
        "\n",
        "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<PAD>` token. It is also a good practice to provide RNN with sequence lengths, so it can skip computations for padding parts. We provide the batching function *batches_generator* readily available for you to save time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gfbY7kQZxb3"
      },
      "source": [
        "def batches_generator(batch_size, tokens, tags,\n",
        "                      shuffle=True, allow_smaller_last_batch=True):\n",
        "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
        "    n_samples = len(tokens)\n",
        "    if shuffle:\n",
        "        order = np.random.permutation(n_samples)\n",
        "    else:\n",
        "        order = np.arange(n_samples)\n",
        "\n",
        "    n_batches = n_samples // batch_size\n",
        "    if allow_smaller_last_batch and n_samples % batch_size:\n",
        "        n_batches += 1\n",
        "\n",
        "    for k in range(n_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k + 1) * batch_size, n_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        max_len_token = 0\n",
        "        for idx in order[batch_start: batch_end]:\n",
        "            x_list.append(words2idxs(tokens[idx]))\n",
        "            y_list.append(tags2idxs(tags[idx]))\n",
        "            max_len_token = max(max_len_token, len(tags[idx]))\n",
        "            \n",
        "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
        "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
        "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
        "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            utt_len = len(x_list[n])\n",
        "\n",
        "            x[n, :utt_len] = x_list[n]\n",
        "            lengths[n] = utt_len\n",
        "            y[n, :utt_len] = y_list[n]\n",
        "        yield x, y, lengths"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bixg69LdbD1"
      },
      "source": [
        "### 1.5. Dictionary Mappings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4hfRUSUZxb1"
      },
      "source": [
        "The next additional functions will help you to create the mapping between tokens and ids for a sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq3h1OFEZxb1"
      },
      "source": [
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        "\n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        "\n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        "\n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXIviMmGQSm"
      },
      "source": [
        "#2. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dWlABhuP0Wf"
      },
      "source": [
        "### 2.1. Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVWg8E9SxjG5"
      },
      "source": [
        "import torch\n",
        "from torch.nn import LSTM\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt \n",
        "import time\n",
        "import math\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3s914lSJeRb"
      },
      "source": [
        "# Model design\n",
        "EMBEDDING_DIM = 200\n",
        "HIDDEN_DIM = 128\n",
        "DROPOUT = 0.5\n",
        "NUM_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "# Training phase\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "LR = 0.01\n",
        "LR_Decay = math.sqrt(2)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKQE11m7L2a9"
      },
      "source": [
        "[This page](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) may help you understand the following code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eafU9ZGMZxb5"
      },
      "source": [
        "### 2.2. Design Network\n",
        "\n",
        "This is the most important part of the assignment. Here we will specify the network architecture based on TensorFlow building blocks. It's fun and easy as a lego constructor! We will create an LSTM network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use Bi-Directional LSTM (Bi-LSTM). Dense layer will be used on top to perform tag classification.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK9Wmbf_JM0E"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                            hidden_dim, \n",
        "                            num_layers=NUM_LAYERS, \n",
        "                            dropout=DROPOUT,\n",
        "                            bidirectional=BIDIRECTIONAL)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        if BIDIRECTIONAL:\n",
        "          self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
        "        else:\n",
        "          self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "                                                    # sentence: [bs x T]\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "                                                      # embeds: [bs x T x E] \n",
        "        embeds = embeds.permute(1, 0, 2)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "                                                          # in: [T x bs x E]\n",
        "                                                         # out: [T x bs x D*n-dir]\n",
        "        preds = self.hidden2tag(lstm_out)\n",
        "                                                       # preds: [T x bs x N]\n",
        "        tag_scores = F.log_softmax(preds, dim=2)\n",
        "        return tag_scores.permute(1, 0, 2), preds.permute(1, 0, 2)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7RqxJp4PcwE"
      },
      "source": [
        "### 2.3. Loss and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOjJv5bEDSJs"
      },
      "source": [
        "During training we do not need predictions of the network, but we need a loss function. We will use [cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropy#torch.nn.CrossEntropyLoss), efficiently implemented in PyTorch. \n",
        "\n",
        "Note that it should be applied to the output of the model before softmax probabilities. Also note,  that we do not want to take into account loss terms coming from `<PAD>` tokens. So we need to mask them out, before computing the loss.\n",
        "\n",
        "\n",
        "Moreover, please note that because the `O` class (not-entity words) is large. To mitigate the imbalance classes, we use weights in the loss function to rescale the loss for each class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42NetX6ucPwf",
        "outputId": "07033002-998e-4fa6-9a6e-0419d9414590"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "flat_train_tags = [item for sublist in train_tags for item in sublist]\n",
        "# class_wts = compute_class_weight('balanced', np.unique(flat_train_tags), flat_train_tags)\n",
        "class_wts = compute_class_weight('balanced', idx2tag, flat_train_tags)\n",
        "print([\"{0:0.2f}\".format(w) for w in class_wts])\n",
        "\n",
        "weights = torch.tensor(class_wts, dtype=torch.float)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['0.05', '20.52', '24.05', '14.97', '8.22', '7.40', '5.37', '6.29', '5.94', '15.16', '11.70', '21.94', '4.78', '20.61', '21.74', '9.88', '70.02', '52.32', '82.09', '82.09', '68.02']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "251BHIyHjRjP",
        "outputId": "8c12f521-cda3-4634-af16-827c8b69944c"
      },
      "source": [
        "len(weights)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Sj21sU_KTs-"
      },
      "source": [
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(token2idx), len(tag2idx))\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# loss_function = nn.CrossEntropyLoss(ignore_index=tag2idx[\"O\"], weight=weights.to(device))\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=tag2idx[\"O\"])\n",
        "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=LR)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeWeHumKJVNm"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBviB9Pmra9-"
      },
      "source": [
        "### 3.1. Train Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmtdehcbNox4"
      },
      "source": [
        "[This page](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) may help you to understand the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57AoslodI8Bh"
      },
      "source": [
        "# TEMP = 2\n",
        "\n",
        "# function to train the model\n",
        "def train():\n",
        "  \n",
        "  model.train()\n",
        "\n",
        "  avg_loss = 0\n",
        "  n = 0\n",
        "  for x_batch, y_batch, lengths in batches_generator(BATCH_SIZE, train_tokens, train_tags):\n",
        "\n",
        "      # Step 1. Remember that Pytorch accumulates gradients.\n",
        "      # We need to clear them out before each instance\n",
        "      model.zero_grad()\n",
        "\n",
        "      # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "      # Tensors of word indices.\n",
        "      sentence_in = torch.LongTensor(x_batch)   # [bs x T]\n",
        "      targets = torch.LongTensor(y_batch)        # [bs x T]\n",
        "\n",
        "      # Step 3. Run our forward pass.\n",
        "      _, preds = model(sentence_in.to(device))                         # [bs x T x N]\n",
        "\n",
        "      # print(\"Train Target size: \", targets.shape)\n",
        "      # print(\"Train preds size: \", preds.shape)\n",
        "\n",
        "      # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "      #  calling optimizer.step()\n",
        "      loss = loss_function(preds.permute(0, 2, 1), targets.to(device))\n",
        "      loss.backward()\n",
        "\n",
        "      # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "      optimizer.step()\n",
        "      \n",
        "      avg_loss += float(loss)\n",
        "      n += 1\n",
        "  \n",
        " \n",
        "  return avg_loss/n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MS2LHnb0-l1"
      },
      "source": [
        "### 3.2. Eval Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG2-Rf5X1B-O"
      },
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "  \n",
        "  # to set in evaluation mode (together with torch.no_grad())\n",
        "  model.eval()\n",
        "  \n",
        "  losses = 0\n",
        "  n = 0\n",
        "  # iterate over batches\n",
        "  for x_batch, y_batch, lengths in batches_generator(BATCH_SIZE, validation_tokens, validation_tags):\n",
        "\n",
        "    sentence_in = torch.LongTensor(x_batch)    # [bs x T]\n",
        "    targets = torch.LongTensor(y_batch)        # [bs x T]\n",
        "\n",
        "    # deactivate autograd\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # model predictions\n",
        "      _, preds = model(sentence_in.to(device))\n",
        "                                                            # [bs x T x N]\n",
        "    # print(\"Validation Target size: \", targets.shape)\n",
        "    # print(\"Validation preds size: \", preds.shape)\n",
        "    \n",
        "    # compute the validation loss between actual and predicted values\n",
        "    loss = loss_function(preds.permute(0, 2, 1), targets.to(device))\n",
        "    losses += float(loss)\n",
        "    n += 1\n",
        "\n",
        "  return losses/n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIxveOUpLgIg"
      },
      "source": [
        "### 3.3. Main Training Loop "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "mxkjRNPDJREN",
        "outputId": "7ed60d9c-7773-426b-f20c-50d6a7bda8b8"
      },
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "tic = time.time()  \n",
        "for epoch in range(EPOCHS): \n",
        "  train_loss = train()\n",
        "  train_losses.append(train_loss)\n",
        "\n",
        "  val_loss = evaluate()\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  print(\"Epoch %d of %d: Train Loss = %.2f, Val Loss = %.2f\" \n",
        "        % (epoch+1, EPOCHS, train_loss, val_loss))\n",
        "  LR = LR / LR_Decay\n",
        "  optimizer = optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "tac = time.time()\n",
        "print(\"Training %d epochs took %d sec. \" % (EPOCHS, (tac-tic)))\n",
        "\n",
        "train_plt, = plt.plot(range(1, EPOCHS+1, 1), train_losses, label=\"train\")\n",
        "plt.title(\"Training Curve\")\n",
        "plt.xlabel(\"Epoch idx\")\n",
        "plt.ylabel(\"Loss\")\n",
        "val_plt, = plt.plot(range(1, EPOCHS+1, 1), val_losses, label=\"val\")\n",
        "plt.legend(handles=[train_plt, val_plt], labels=[\"train\", \"validation\"])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 of 20: Train Loss = 3.01, Val Loss = 2.99\n",
            "Epoch 2 of 20: Train Loss = 2.96, Val Loss = 2.96\n",
            "Epoch 3 of 20: Train Loss = 2.95, Val Loss = 2.94\n",
            "Epoch 4 of 20: Train Loss = 2.92, Val Loss = 2.92\n",
            "Epoch 5 of 20: Train Loss = 2.92, Val Loss = 2.91\n",
            "Epoch 6 of 20: Train Loss = 2.92, Val Loss = 2.91\n",
            "Epoch 7 of 20: Train Loss = 2.91, Val Loss = 2.90\n",
            "Epoch 8 of 20: Train Loss = 2.91, Val Loss = 2.90\n",
            "Epoch 9 of 20: Train Loss = 2.90, Val Loss = 2.90\n",
            "Epoch 10 of 20: Train Loss = 2.89, Val Loss = 2.90\n",
            "Epoch 11 of 20: Train Loss = 2.90, Val Loss = 2.90\n",
            "Epoch 12 of 20: Train Loss = 2.90, Val Loss = 2.90\n",
            "Epoch 13 of 20: Train Loss = 2.90, Val Loss = 2.90\n",
            "Epoch 14 of 20: Train Loss = 2.88, Val Loss = 2.90\n",
            "Epoch 15 of 20: Train Loss = 2.90, Val Loss = 2.89\n",
            "Epoch 16 of 20: Train Loss = 2.90, Val Loss = 2.89\n",
            "Epoch 17 of 20: Train Loss = 2.88, Val Loss = 2.89\n",
            "Epoch 18 of 20: Train Loss = 2.90, Val Loss = 2.89\n",
            "Epoch 19 of 20: Train Loss = 2.90, Val Loss = 2.89\n",
            "Epoch 20 of 20: Train Loss = 2.90, Val Loss = 2.89\n",
            "Training 20 epochs took 55 sec. \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-80abFll1OUh"
      },
      "source": [
        "# 4. Test\n",
        "We should get predictions for test data (without updating parameters) to evaluation the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpupVhvWUchZ"
      },
      "source": [
        "# from sklearn.metrics import classification_report\n",
        "from evaluation import precision_recall_f1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0T9BrebOoF_"
      },
      "source": [
        "y_true, y_pred = [], []\n",
        "for x_batch, y_batch, lengths in batches_generator(BATCH_SIZE, test_tokens, test_tags):\n",
        "\n",
        "  with torch.no_grad():\n",
        "    preds, _ = model(torch.LongTensor(x_batch).to(device))            # [bs x T x N]\n",
        "\n",
        "  preds = np.argmax(preds.cpu().numpy(), axis=2)                 # [bs x T]\n",
        "\n",
        "  for target_tags, pred_tags, tokens in zip (y_batch, preds, x_batch):\n",
        "    for target_tag, pred_tag, token in zip (target_tags, pred_tags, tokens):\n",
        "      if token != token2idx[\"<PAD>\"]:\n",
        "        y_true.append(target_tag)    \n",
        "        y_pred.append(pred_tag)\n",
        "\n",
        "precision_recall_f1(idxs2tags(y_true), idxs2tags(y_pred), print_results=True, short_report=True)\n",
        "# print(\"test prediction shape after argmax: \", len(preds))\n",
        "# print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnDgcNTvpjWx"
      },
      "source": [
        "# TODOs\n",
        "\n",
        "- [x] Running on the GPU\n",
        "- [x] Plot training loss curve\n",
        "- [x] Mini-batch processing\n",
        "  * Check dimensions\n",
        "  * Padding to the max length\n",
        "- [x] Loss function\n",
        "  * Cross entropy loss\n",
        "  * Applying mask to the loss\n",
        "  * Applying class weights to the loss function\n",
        "- [x] Evaluate on val/test set by Precision, Recall, F1 metrics\n",
        "- [x] Dynamic learning rate\n",
        "- [x] Using BiLSTM\n",
        "- [X] Reqularizations such as dropout\n",
        "- [ ] Hyperparameter tuning\n",
        "  * EMBEDDING_DIM\n",
        "  * HIDDEN_DIM\n",
        "  * Batch size\n",
        "  * Number of epochs\n",
        "  * Optimizer algorithm\n",
        "  * ...\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ON8vx_lIcx4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}