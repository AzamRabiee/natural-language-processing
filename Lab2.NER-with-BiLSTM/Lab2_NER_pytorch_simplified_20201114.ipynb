{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.4.3"
    },
    "colab": {
      "name": "Lab2-NER-pytorch-simplified-20201114.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "tUC02h2AZxbf"
      },
      "source": [
        "# Recognize named entities on Twitter with LSTMs\n",
        "\n",
        "In this assignment, you will use a recurrent neural network to solve Named Entity Recognition (NER) problem. NER is a common task in natural language processing systems. It serves for extraction such entities from the text as persons, organizations, locations, etc. In this task you will experiment to recognize named entities from Twitter.\n",
        "\n",
        "For example, we want to extract persons' and organizations' names from the text. Than for the input text:\n",
        "\n",
        "    Ian Goodfellow works for Google Brain\n",
        "\n",
        "a NER model needs to provide the following sequence of tags:\n",
        "\n",
        "    B-PER I-PER    O     O   B-ORG  I-ORG\n",
        "\n",
        "Where *B-* and *I-* prefixes stand for the beginning and inside of the entity, while *O* stands for out of tag or no tag. Markup with the prefix scheme is called *BIO markup*. This markup is introduced for distinguishing of consequent entities with similar types.\n",
        "\n",
        "A solution of the task will be based on neural networks, particularly, on Bi-Directional Long Short-Term Memory Networks (Bi-LSTMs).\n",
        "\n",
        "### Libraries\n",
        "\n",
        "For this task you will need the following libraries:\n",
        " - [pytorch](https://pytorch.org/) — an open-source software library for Machine Intelligence.\n",
        "    \n",
        " - [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfFm1a75Zxbg"
      },
      "source": [
        "### Data\n",
        "\n",
        "The following cell will download all data required for this assignment into the folder `./data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OnOcAweZxbh"
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    ! wget https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py -O setup_google_colab.py\n",
        "    import setup_google_colab\n",
        "    setup_google_colab.setup_week2()\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from common.download_utils import download_week2_resources\n",
        "\n",
        "download_week2_resources()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9rmHTRHZxbk"
      },
      "source": [
        "### Load the Twitter Named Entity Recognition corpus\n",
        "\n",
        "We will work with a corpus, which contains tweets with NE tags. Every line of a file contains a pair of a token (word/punctuation symbol) and a tag, separated by a whitespace. Different tweets are separated by an empty line.\n",
        "\n",
        "The function *read_data* reads a corpus from the *file_path* and returns two lists: one with tokens and one with the corresponding tags. You need to complete this function by adding a code, which will replace a user's nickname to `<USR>` token and any URL to `<URL>` token. You could think that a URL and a nickname are just strings which start with *http://* or *https://* in case of URLs and a *@* symbol for nicknames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1auNuazZxbl"
      },
      "source": [
        "def read_data(file_path):\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    \n",
        "    tweet_tokens = []\n",
        "    tweet_tags = []\n",
        "    for line in open(file_path, encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if tweet_tokens:\n",
        "                tokens.append(tweet_tokens)\n",
        "                tags.append(tweet_tags)\n",
        "            tweet_tokens = []\n",
        "            tweet_tags = []\n",
        "        else:\n",
        "            token, tag = line.split()\n",
        "            # Replace all urls with <URL> token\n",
        "            # Replace all users with <USR> token\n",
        "\n",
        "            ######################################\n",
        "            ######### YOUR CODE HERE #############\n",
        "            ######################################\n",
        "                \n",
        "            tweet_tokens.append(token)\n",
        "            tweet_tags.append(tag)\n",
        "            \n",
        "    return tokens, tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k47H-84bZxbn"
      },
      "source": [
        "And now we can load three separate parts of the dataset:\n",
        " - *train* data for training the model;\n",
        " - *validation* data for evaluation and hyperparameters tuning;\n",
        " - *test* data for final evaluation of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuzybiCvZxbo"
      },
      "source": [
        "train_tokens, train_tags = read_data('data/train.txt')\n",
        "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
        "test_tokens, test_tags = read_data('data/test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEvX7kivZxbq"
      },
      "source": [
        "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssJtjq-AZxbr"
      },
      "source": [
        "for i in range(2):\n",
        "    for token, tag in zip(train_tokens[i], train_tags[i]):\n",
        "        print('%s\\t%s' % (token, tag))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK59OTpwZxbt"
      },
      "source": [
        "### Prepare dictionaries\n",
        "\n",
        "To train a neural network, we will use two mappings: \n",
        "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
        "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
        "\n",
        "Now you need to implement the function *build_dict* which will return {token or tag}$\\to${index} and vice versa. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo7tkqv8Zxbt"
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfUx5jYZZxbw"
      },
      "source": [
        "def build_dict(tokens_or_tags, special_tokens):\n",
        "    \"\"\"\n",
        "        tokens_or_tags: a list of lists of tokens or tags\n",
        "        special_tokens: some special tokens\n",
        "    \"\"\"\n",
        "    # Create a dictionary with default value 0\n",
        "    # The lambda function gets called whenever it needs a default value.\n",
        "\n",
        "    tok2idx = defaultdict(lambda: 0)\n",
        "    idx2tok = []\n",
        "    \n",
        "    # Create mappings from tokens (or tags) to indices and vice versa.\n",
        "    # At first, add special tokens (or tags) to the dictionaries.\n",
        "    # The first special token must have index 0.\n",
        "    ######################################\n",
        "    ######### YOUR CODE HERE #############\n",
        "    ######################################\n",
        "    \n",
        "    # Mapping tok2idx should contain each token or tag only once. \n",
        "    # To do so, you should:\n",
        "    # 1. extract unique tokens/tags from the tokens_or_tags variable, which is not\n",
        "    #    occur in special_tokens (because they could have non-empty intersection)\n",
        "    # 2. index them (for example, you can add them into the list idx2tok\n",
        "    # 3. for each token/tag save the index into tok2idx).\n",
        "    \n",
        "    ######################################\n",
        "    ######### YOUR CODE HERE #############\n",
        "    ######################################\n",
        "    return tok2idx, idx2tok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5iG1f1VZxby"
      },
      "source": [
        "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens in our case will be:\n",
        " - `<UNK>` token for out of vocabulary tokens;\n",
        " - `<PAD>` token for padding sentence to the same length when we create batches of sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEswFgQAZxbz"
      },
      "source": [
        "special_tokens = ['<UNK>', '<PAD>']\n",
        "special_tags = ['O']\n",
        "\n",
        "# Create dictionaries \n",
        "token2idx, idx2token = build_dict(train_tokens + validation_tokens, special_tokens)\n",
        "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4hfRUSUZxb1"
      },
      "source": [
        "The next additional functions will help you to create the mapping between tokens and ids for a sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq3h1OFEZxb1"
      },
      "source": [
        "def words2idxs(tokens_list):\n",
        "    return [token2idx[word] for word in tokens_list]\n",
        "\n",
        "def tags2idxs(tags_list):\n",
        "    return [tag2idx[tag] for tag in tags_list]\n",
        "\n",
        "def idxs2words(idxs):\n",
        "    return [idx2token[idx] for idx in idxs]\n",
        "\n",
        "def idxs2tags(idxs):\n",
        "    return [idx2tag[idx] for idx in idxs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eafU9ZGMZxb5"
      },
      "source": [
        "## Build a recurrent neural network\n",
        "\n",
        "This is the most important part of the assignment. Here we will specify the network architecture based on TensorFlow building blocks. It's fun and easy as a lego constructor! We will create an LSTM network which will produce probability distribution over tags for each token in a sentence. To take into account both right and left contexts of the token, we will use Bi-Directional LSTM (Bi-LSTM). Dense layer will be used on top to perform tag classification.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVWg8E9SxjG5"
      },
      "source": [
        "import torch\n",
        "from torch.nn import LSTM\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7b8NOZpLXYl"
      },
      "source": [
        "[This page](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) may help you understand the following code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXko-Y-RO9qo"
      },
      "source": [
        "[This page](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) may help you to understand the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK9Wmbf_JM0E"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = ######### YOUR CODE HERE #############\n",
        "\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = ######### YOUR CODE HERE #############\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeWeHumKJVNm"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3s914lSJeRb"
      },
      "source": [
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VpvalQA9i31"
      },
      "source": [
        "Create LSTMTagger model with the following parameters:\n",
        "* embedding_dim — dimension of embeddings, recommended value: 200, \n",
        "* hidden_dim — size of hidden layers for RNN, recommended value: 200;\n",
        "* vocab_size — number of tokens;\n",
        "* tagset_size — number of tags;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Sj21sU_KTs-"
      },
      "source": [
        "model = ######### YOUR CODE HERE #############\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phk7uD4dKZmU"
      },
      "source": [
        "See what the scores are before training\n",
        "Note that element i,j of the output is the score for tag j for word i.\n",
        "Here we don't need to train, so the code is wrapped in torch.no_grad()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4mWjUpJKekf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca758b9-9368-4a7b-b695-ff35b99296df"
      },
      "source": [
        "with torch.no_grad():\n",
        "    inputs = torch.tensor(words2idxs(train_tokens[0]))\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "    print(tag_scores[6:9])\n",
        "    print(tags2idxs(train_tags[0])[6:9])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.9308, -1.5112, -1.8668, -2.0909, -2.1039, -1.4519],\n",
            "        [-1.9598, -1.4916, -1.9578, -2.1166, -2.0857, -1.3932],\n",
            "        [-1.9082, -1.5217, -1.9122, -2.1426, -2.1020, -1.4025]])\n",
            "[0, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBviB9Pmra9-"
      },
      "source": [
        "### Simplified Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxkjRNPDJREN"
      },
      "source": [
        "for epoch in range(300): \n",
        "    for sentence, tags in zip(train_tokens[:2], train_tags[:2]):\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "        # Tensors of word indices.\n",
        "        sentence_in = ######### YOUR CODE HERE #############\n",
        "        targets = ######### YOUR CODE HERE #############\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss = ######### YOUR CODE HERE #############\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqcRlEuXN0K8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d92a4b-5386-44e7-9d02-d39a89b5af8f"
      },
      "source": [
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "    inputs = torch.tensor(words2idxs(train_tokens[0]))\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "    print(train_tokens[0][6:9])\n",
        "    print(tag_scores.shape)\n",
        "    for score in tag_scores.numpy()[6:9]:\n",
        "      print([ \"%.2f\" % s for s in score])\n",
        "\n",
        "    print(tags2idxs(train_tags[0])[6:9])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['for', 'Ghostland', 'Observatory']\n",
            "torch.Size([26, 6])\n",
            "['-0.22', '-4.20', '-3.95', '-4.31', '-2.08', '-3.69']\n",
            "['-1.05', '-2.06', '-2.63', '-2.73', '-1.32', '-2.15']\n",
            "['-0.92', '-2.27', '-1.97', '-2.82', '-1.92', '-1.89']\n",
            "[0, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnDgcNTvpjWx"
      },
      "source": [
        "# TODOs\n",
        "\n",
        "\n",
        "* Plot training loss curve\n",
        "* Mini-batch processing\n",
        "  * check dimensions\n",
        "  * Padding to the max length\n",
        "* Loss function\n",
        "  * Cross entropy loss\n",
        "  * Applying mask to the loss \n",
        "* Dynamic learning rate\n",
        "* Evaluate on val/test set by Precision, Recall, F1 metrics\n",
        "* Hyperparameter tuning:\n",
        "  * EMBEDDING_DIM\n",
        "  * HIDDEN_DIM\n",
        "  * batch size\n",
        "  * number of epochs\n",
        "  * ...\n",
        "* Apply BiLSTM\n",
        "* Try Adam optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oM0RI6-Zxb3"
      },
      "source": [
        "### Generate batches\n",
        "\n",
        "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<PAD>` token. It is also a good practice to provide RNN with sequence lengths, so it can skip computations for padding parts. We provide the batching function *batches_generator* readily available for you to save time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gfbY7kQZxb3"
      },
      "source": [
        "def batches_generator(batch_size, tokens, tags,\n",
        "                      shuffle=True, allow_smaller_last_batch=True):\n",
        "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
        "    print(\"batch size: \", batch_size)\n",
        "    print(\"tokens len: \", len(tokens))\n",
        "    print(\"tags len: \", len(tags))\n",
        "\n",
        "    n_samples = len(tokens)\n",
        "    if shuffle:\n",
        "        order = np.random.permutation(n_samples)\n",
        "    else:\n",
        "        order = np.arange(n_samples)\n",
        "\n",
        "    n_batches = n_samples // batch_size\n",
        "    if allow_smaller_last_batch and n_samples % batch_size:\n",
        "        n_batches += 1\n",
        "\n",
        "    for k in range(n_batches):\n",
        "        batch_start = k * batch_size\n",
        "        batch_end = min((k + 1) * batch_size, n_samples)\n",
        "        current_batch_size = batch_end - batch_start\n",
        "        x_list = []\n",
        "        y_list = []\n",
        "        max_len_token = 0\n",
        "        for idx in order[batch_start: batch_end]:\n",
        "            x_list.append(words2idxs(tokens[idx]))\n",
        "            y_list.append(tags2idxs(tags[idx]))\n",
        "            max_len_token = max(max_len_token, len(tags[idx]))\n",
        "            \n",
        "        # Fill in the data into numpy nd-arrays filled with padding indices.\n",
        "        x = np.ones([current_batch_size, max_len_token], dtype=np.int32) * token2idx['<PAD>']\n",
        "        y = np.ones([current_batch_size, max_len_token], dtype=np.int32) * tag2idx['O']\n",
        "        lengths = np.zeros(current_batch_size, dtype=np.int32)\n",
        "        for n in range(current_batch_size):\n",
        "            utt_len = len(x_list[n])\n",
        "            x[n, :utt_len] = x_list[n]\n",
        "            lengths[n] = utt_len\n",
        "            y[n, :utt_len] = y_list[n]\n",
        "        yield x, y, lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0T9BrebOoF_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}